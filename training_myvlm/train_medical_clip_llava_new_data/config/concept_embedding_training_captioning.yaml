concept_name: hyoidbone
concept_identifier: hyoidbone         #sks   # For people, this should be a short name such as Bob or Anna
concept_type: OBJECT               # Can also be PERSON
vlm_type: LLAVA
personalization_task: CAPTIONING   # For BLIP-2, we only focus on CAPTIONING, but VQA is also possible
output_root: /home/user_7734/training_myvlm/train_medical_clip_llava_new_data/output_concept_embedding_training
data_root: /home/user_7734/training_myvlm/train_medical_clip_llava_new_data/data/pos_hyoidbone
concept_head_path: /home/user_7734/training_myvlm/train_medical_clip_llava_new_data/trained_concept_head
threshold: 0.5
optimization_steps: 90
learning_rate: 0.0001 # omer changed because exploded
batch_size: 4 #4 #omer changed because A6000 isnt enough -_-
reg_lambda: 0.1
save_interval: 25
val_interval: 10
seed: 42           # If concept is an object, this should be the same seed that was used for training the concept heads
device: cuda
torch_dtype: bfloat16
