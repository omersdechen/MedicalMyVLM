concept_name: my_cat
concept_identifier: sks            # For people, this should be a short name such as Bob or Anna
concept_type: OBJECT               # Can also be PERSON
vlm_type: LLAVA
personalization_task: CAPTIONING   # For BLIP-2, we only focus on CAPTIONING, but VQA is also possible
output_root: /home/user_7734/training_myvlm/train_basic_clip_llava/output_concept_embedding_training
data_root: /home/user_7734/training_myvlm/train_basic_clip_llava/data/pos_cat
concept_head_path: /home/user_7734/training_myvlm/train_basic_clip_llava/trained_concept_head
threshold: 0.5
optimization_steps: 100
learning_rate: 1.0 # omer changed because exploded
batch_size: 3 #4 #omer changed because A6000 isnt enough -_-
reg_lambda: 0.0075
save_interval: 25
val_interval: 25
seed: 42           # If concept is an object, this should be the same seed that was used for training the concept heads
device: cuda
torch_dtype: bfloat16
